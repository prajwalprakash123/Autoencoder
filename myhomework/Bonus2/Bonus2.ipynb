{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Bonus2.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHidGAzwhD8r"
      },
      "source": [
        "# Bonus2: Build a Supervised Autoencoder.\n",
        "\n",
        "### Name: [Prajwal Prakash]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN-VHiH7hD8x"
      },
      "source": [
        "\n",
        "PCA and the standard autoencoder are unsupervised dimensionality reduction methods, and their learned features are not discriminative. If you build a classifier upon the low-dimenional features extracted by PCA and autoencoder, you will find the classification accuracy very poor.\n",
        "\n",
        "Linear discriminant analysis (LDA) is a traditionally supervised dimensionality reduction method for learning low-dimensional features which are highly discriminative. Likewise, can we extend autoencoder to supervised leanring?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V9TTe6thD8x"
      },
      "source": [
        "**You are required to build and train a supervised autoencoder look like the following.** You are required to add other layers properly to alleviate overfitting.\n",
        "\n",
        "\n",
        "![Network Structure](https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/supervised_ae.png?raw=true \"NetworkStructure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiML1MdFhD8y"
      },
      "source": [
        "## 0. You will do the following:\n",
        "\n",
        "1. Read and run my code to train a standard dense autoencoder.\n",
        "\n",
        "2. Build and train a supervised autoencoder, visual the low-dim features and the reconstructions, and evaluate whether the learned low-dim features are discriminative.\n",
        "    \n",
        "3. Convert the .IPYNB file to .HTML file.\n",
        "\n",
        "    * The HTML file must contain the code and the output after execution.\n",
        "    \n",
        "    \n",
        "    \n",
        "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo.\n",
        "\n",
        "4. Submit the link to this .HTML file to Canvas.\n",
        "\n",
        "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/Bonus2/Bonus2.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvSrag1JhD8y"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAsOY4bRhD8y"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI9RgswLhD8z"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 28*28).astype('float32') / 255.\n",
        "x_test = x_test.reshape(10000, 28*28).astype('float32') / 255.\n",
        "\n",
        "print('Shape of x_train: ' + str(x_train.shape)) \n",
        "print('Shape of x_test: ' + str(x_test.shape))\n",
        "print('Shape of y_train: ' + str(y_train.shape))\n",
        "print('Shape of y_test: ' + str(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkbrmuothD8z"
      },
      "source": [
        "### 1.2. One-hot encode the labels\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBVEswP4hD80"
      },
      "source": [
        "import numpy\n",
        "\n",
        "def to_one_hot(y, num_class=10):\n",
        "    results = numpy.zeros((len(y), num_class))\n",
        "    for i, label in enumerate(y):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXtCBm75hD80"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets\n",
        "\n",
        "Randomly partition the 60K training samples to 2 sets:\n",
        "* a training set containing 10K samples;\n",
        "* a validation set containing 50K samples. (You can use only 10K to save time.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiZ-FDw-hD80"
      },
      "source": [
        "rand_indices = numpy.random.permutation(60000)\n",
        "train_indices = rand_indices[0:10000]\n",
        "valid_indices = rand_indices[10000:20000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-dr5YCUhD81"
      },
      "source": [
        "## 2. Build an unsupervised  autoencoder and tune its hyper-parameters\n",
        "\n",
        "1. Build a dense autoencoder model\n",
        "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
        "    * Do NOT use test data for hyper-parameter tuning!!!\n",
        "    \n",
        "3. Try to achieve a validation loss as low as possible.\n",
        "4. Evaluate the model on the test set.\n",
        "5. Visualize the low-dim features and reconstructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Krc6YrZchD81"
      },
      "source": [
        "### 2.1. Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGTSYUoahD81"
      },
      "source": [
        "from keras.layers import Dense, Input\n",
        "from keras import models\n",
        "\n",
        "input_img = Input(shape=(784,), name='input_img')\n",
        "\n",
        "encode1 = Dense(128, activation='relu', name='encode1')(input_img)\n",
        "encode2 = Dense(32, activation='relu', name='encode2')(encode1)\n",
        "encode3 = Dense(8, activation='relu', name='encode3')(encode2)\n",
        "bottleneck = Dense(2, activation='relu', name='bottleneck')(encode3)\n",
        "decode1 = Dense(8, activation='relu', name='decode1')(bottleneck)\n",
        "decode2 = Dense(32, activation='relu', name='decode2')(decode1)\n",
        "decode3 = Dense(128, activation='relu', name='decode3')(decode2)\n",
        "decode4 = Dense(784, activation='relu', name='decode4')(decode3)\n",
        "\n",
        "ae = models.Model(input_img, decode4)\n",
        "\n",
        "ae.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwyfZiwUhD82"
      },
      "source": [
        "# print the network structure to a PDF file\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot, plot_model\n",
        "\n",
        "SVG(model_to_dot(ae, show_shapes=False).create(prog='dot', format='svg'))\n",
        "\n",
        "plot_model(\n",
        "    model=ae, show_shapes=False,\n",
        "    to_file='unsupervised_ae.pdf'\n",
        ")\n",
        "\n",
        "# you can find the file \"unsupervised_ae.pdf\" in the current directory."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO6WeL5WhD82"
      },
      "source": [
        "### 2.2. Train the model and tune the hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56xbDUCDhD82"
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "learning_rate = 1E-3 # to be tuned!\n",
        "\n",
        "ae.compile(loss='mean_squared_error',\n",
        "           optimizer=optimizers.RMSprop(lr=learning_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHKJ14-XhD83"
      },
      "source": [
        "history = ae.fit(x_tr, x_tr, \n",
        "                 batch_size=128, \n",
        "                 epochs=100, \n",
        "                 validation_data=(x_val, x_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce5OoW48hD83"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g15ZgEAzhD83"
      },
      "source": [
        "### 2.3. Visualize the reconstructed test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rPtV_dxhD83"
      },
      "source": [
        "ae_output = ae.predict(x_test).reshape((10000, 28, 28))\n",
        "\n",
        "ROW = 5\n",
        "COLUMN = 4\n",
        "\n",
        "x = ae_output\n",
        "fname = 'reconstruct_ae.pdf'\n",
        "\n",
        "fig, axes = plt.subplots(nrows=ROW, ncols=COLUMN, figsize=(8, 10))\n",
        "for ax, i in zip(axes.flat, numpy.arange(ROW*COLUMN)):\n",
        "    image = x[i].reshape(28, 28)\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(fname)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlwef4yOhD84"
      },
      "source": [
        "### 2.4. Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv5gogDuhD84"
      },
      "source": [
        "loss = ae.evaluate(x_test, x_test)\n",
        "print('loss = ' + str(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4ew3WTThD84"
      },
      "source": [
        "### 2.5. Visualize the low-dimensional features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRSDgdv-hD84"
      },
      "source": [
        "# build the encoder network\n",
        "ae_encoder = models.Model(input_img, bottleneck)\n",
        "ae_encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmYrWZW5hD85"
      },
      "source": [
        "# extract low-dimensional features from the test data\n",
        "encoded_test = ae_encoder.predict(x_test)\n",
        "print('Shape of encoded_test: ' + str(encoded_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HHu5_D-hD85"
      },
      "source": [
        "colors = numpy.array(['r', 'g', 'b', 'm', 'c', 'k', 'y', 'purple', 'darkred', 'navy'])\n",
        "colors_test = colors[y_test]\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.scatter(encoded_test[:, 0], encoded_test[:, 1], s=10, c=colors_test, edgecolors=colors_test)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "fname = 'ae_code.pdf'\n",
        "plt.savefig(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4fp3ZsqhD85"
      },
      "source": [
        "#### Remark:\n",
        "\n",
        "Judging from the visualization, the low-dim features seems not discriminative, as 2D features from different classes are mixed. Let quantatively find out whether they are discriminative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g_Yvw2PhD85"
      },
      "source": [
        "## 3. Are the learned low-dim features discriminative?\n",
        "\n",
        "To find the answer, lets train a classifier on the training set (the extracted 2-dim features) and evaluation on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRbR7LPxhD85"
      },
      "source": [
        "# extract the 2D features from the training, validation, and test samples\n",
        "f_tr = ae_encoder.predict(x_tr)\n",
        "f_val = ae_encoder.predict(x_val)\n",
        "f_te = ae_encoder.predict(x_test)\n",
        "\n",
        "print('Shape of f_tr: ' + str(f_tr.shape))\n",
        "print('Shape of f_te: ' + str(f_te.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvLsjHjAhD86"
      },
      "source": [
        "from keras.layers import Dense, Input\n",
        "from keras import models\n",
        "\n",
        "input_feat = Input(shape=(2,))\n",
        "\n",
        "hidden1 = Dense(128, activation='relu')(input_feat)\n",
        "hidden2 = Dense(128, activation='relu')(hidden1)\n",
        "output = Dense(10, activation='softmax')(hidden2)\n",
        "\n",
        "classifier = models.Model(input_feat, output)\n",
        "\n",
        "classifier.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1_JmnEchD86"
      },
      "source": [
        "classifier.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizers.RMSprop(lr=1E-4),\n",
        "                  metrics=['acc'])\n",
        "\n",
        "history = classifier.fit(f_tr, y_tr, \n",
        "                        batch_size=32, \n",
        "                        epochs=30, \n",
        "                        validation_data=(f_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJU4oPWfhD86"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Using the 2D features, the validation accuracy is 60~70%. Recall that using the original data, the accuracy is about 98%. Obviously, the 2D features are not very discriminative.\n",
        "\n",
        "We are going to build a supervised autoencode model for learning low-dimensional discriminative features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbPIesOahD86"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyboj1vJhD87"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f85SExnyhD87"
      },
      "source": [
        "## 4. Build a supervised autoencoder model\n",
        "\n",
        "\n",
        "**You are required to build and train a supervised autoencoder look like the following.** (Not necessary the same.) You are required to add other layers properly to alleviate overfitting.\n",
        "\n",
        "\n",
        "![Network Structure](https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/supervised_ae.png?raw=true \"NetworkStructure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkWRyoX_hD87"
      },
      "source": [
        "### 4.1. Build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38ib9FKthD87"
      },
      "source": [
        "# build the supervised autoencoder network\n",
        "from keras.layers import Dense, Input\n",
        "from keras import models\n",
        "\n",
        "input_img = Input(shape=(784,), name='input_img')\n",
        "\n",
        "# encoder network\n",
        "encode1 = <add a dense layer taking input_img as input>\n",
        "<Add more layers...>\n",
        "# The width of the bottleneck layer must be exactly 2.\n",
        "bottleneck = <the output of encoder network>\n",
        "\n",
        "# decoder network\n",
        "decode1 = <add a dense layer taking bottleneck as input>\n",
        "<Add more layers...>\n",
        "decode4 = <the output of decoder network>\n",
        "\n",
        "# build a classifier upon the bottleneck layer\n",
        "classifier1 = <add a dense layer taking bottleneck as input>\n",
        "<Add more dense layers and regularizations...>\n",
        "classifier3 = <the output of classifier network>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hd-fg9OhD88"
      },
      "source": [
        "# connect the input and the two outputs\n",
        "sae = models.Model(input_img, [decode4, classifier3])\n",
        "\n",
        "sae.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VebvJ_3PhD88"
      },
      "source": [
        "# print the network structure to a PDF file\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot, plot_model\n",
        "\n",
        "SVG(model_to_dot(sae, show_shapes=False).create(prog='dot', format='svg'))\n",
        "\n",
        "plot_model(\n",
        "    model=sae, show_shapes=False,\n",
        "    to_file='supervised_ae.pdf'\n",
        ")\n",
        "\n",
        "# you can find the file \"supervised_ae.pdf\" in the current directory."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcR_PuZdhD88"
      },
      "source": [
        "### 4.2. Train the new model and tune the hyper-parameters\n",
        "\n",
        "The new model has multiple output. Thus we specify **multiple** loss functions and their weights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwwxePC6hD88"
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "sae.compile(loss=['mean_squared_error', 'categorical_crossentropy'],\n",
        "            loss_weights=[1, 0.5], # to be tuned\n",
        "            optimizer=optimizers.RMSprop(lr=1E-3))\n",
        "\n",
        "history = sae.fit(x_tr, [x_tr, y_tr], \n",
        "                  batch_size=32, \n",
        "                  epochs=100, \n",
        "                  validation_data=(x_val, [x_val, y_val]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNGqyyFjhD89"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y69rYYNhD89"
      },
      "source": [
        "### Question\n",
        "\n",
        "Do you think overfitting is happening? If yes, what can you do? Please make necessary changes to the supervised autoencoder network structure.\n",
        "\n",
        "**Failing to add proper regularization will lose 1~2 scores.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM6W6-TAhD89"
      },
      "source": [
        "### 4.3. Visualize the reconstructed test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NowMFtfdhD89"
      },
      "source": [
        "sae_output = sae.predict(x_test)[0].reshape((10000, 28, 28))\n",
        "\n",
        "ROW = 5\n",
        "COLUMN = 4\n",
        "\n",
        "x = sae_output\n",
        "fname = 'reconstruct_sae.pdf'\n",
        "\n",
        "fig, axes = plt.subplots(nrows=ROW, ncols=COLUMN, figsize=(8, 10))\n",
        "for ax, i in zip(axes.flat, numpy.arange(ROW*COLUMN)):\n",
        "    image = x[i].reshape(28, 28)\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(fname)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGx015aGhD89"
      },
      "source": [
        "### 4.4. Visualize the low-dimensional features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTMIkxyuhD89"
      },
      "source": [
        "# build the encoder model\n",
        "sae_encoder = models.Model(input_img, bottleneck)\n",
        "sae_encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mGOPm_2thD8-"
      },
      "source": [
        "# extract test features\n",
        "encoded_test = sae_encoder.predict(x_test)\n",
        "print('Shape of encoded_test: ' + str(encoded_test.shape))\n",
        "\n",
        "colors = numpy.array(['r', 'g', 'b', 'm', 'c', 'k', 'y', 'purple', 'darkred', 'navy'])\n",
        "colors_test = colors[y_test]\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.scatter(encoded_test[:, 0], encoded_test[:, 1], s=10, c=colors_test, edgecolors=colors_test)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "fname = 'sae_code.pdf'\n",
        "plt.savefig(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yetHd9uthD8-"
      },
      "source": [
        "### 4.5. Are the learned low-dim features discriminative?\n",
        "\n",
        "To find the answer, lets train a classifier on the training set (the extracted 2-dim features) and evaluation on the validation and test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MQjoqIkhD8-"
      },
      "source": [
        "# extract 2D features from the training, validation, and test samples\n",
        "f_tr = sae_encoder.predict(x_tr)\n",
        "f_val = sae_encoder.predict(x_val)\n",
        "f_te = sae_encoder.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHXS36NshD8-"
      },
      "source": [
        "# build a classifier which takes the 2D features as input\n",
        "from keras.layers import Dense, Input\n",
        "from keras import models\n",
        "\n",
        "input_feat = Input(shape=(2,))\n",
        "\n",
        "<build a classifier which takes input_feat as input>\n",
        "output = <output of the classifier network>\n",
        "\n",
        "classifier = models.Model(input_feat, output)\n",
        "\n",
        "classifier.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJzjXknAhD8_"
      },
      "source": [
        "classifier.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizers.RMSprop(lr=1E-4),\n",
        "                  metrics=['acc'])\n",
        "\n",
        "history = classifier.fit(f_tr, y_tr, \n",
        "                        batch_size=32, \n",
        "                        epochs=30, \n",
        "                        validation_data=(f_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMsUsdfLhD8_"
      },
      "source": [
        "#### Remark:\n",
        "\n",
        "The validation accuracy must be above 90%. It means the low-dim features learned by the supervised autoencoder are very effective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYWm754JhD8_"
      },
      "source": [
        "# evaluate your model on the never-seen-before test data\n",
        "# write your code here:\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}